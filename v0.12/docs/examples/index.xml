<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HAProxy Ingress â€“ Examples</title>
    <link>/v0.12/docs/examples/</link>
    <description>Recent content in Examples on HAProxy Ingress</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="/v0.12/docs/examples/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Blue/green</title>
      <link>/v0.12/docs/examples/blue-green/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/v0.12/docs/examples/blue-green/</guid>
      <description>
        
        
        

&lt;p&gt;This example demonstrates how to configure
&lt;a href=&#34;https://www.martinfowler.com/bliki/BlueGreenDeployment.html&#34;&gt;blue/green deployment&lt;/a&gt;
on HAProxy Ingress controller, in order to route requests based on distict weight on
deployment groups as well as selecting a group based on http header or cookie value.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;This document has the following prerequisite:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A Kubernetes cluster with a running HAProxy Ingress controller v0.6 or above.
See the &lt;a href=&#34;https://github.com/jcmoraisjr/haproxy-ingress/tree/master/examples/setup-cluster.md#five-minutes-deployment&#34;&gt;five minutes deployment&lt;/a&gt;
or the &lt;a href=&#34;https://github.com/jcmoraisjr/haproxy-ingress/tree/master/examples/deployment&#34;&gt;deployment example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;deploying-applications&#34;&gt;Deploying applications&lt;/h2&gt;

&lt;p&gt;In order to the configuration have effect, at least two deployments, or daemon sets, or replication
controllers should be used with at least two pairs of label name/value.&lt;/p&gt;

&lt;p&gt;The following instructions create two deployment objects using &lt;code&gt;run&lt;/code&gt; label as the service selector
and &lt;code&gt;group&lt;/code&gt; label as the blue/green deployment selector:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run blue \
  --image=jcmoraisjr/whoami \
  --port=8000 --labels=run=bluegreen,group=blue
deployment &amp;quot;blue&amp;quot; created

$ kubectl run green \
  --image=jcmoraisjr/whoami \
  --port=8000 --labels=run=bluegreen,group=green
deployment &amp;quot;green&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Certify that the pods are running and have the correct labels. Note that both &lt;code&gt;group&lt;/code&gt; and &lt;code&gt;run&lt;/code&gt;
labels were applied:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pod -lrun=bluegreen --show-labels
NAME                     READY     STATUS    RESTARTS   AGE       LABELS
blue-79c9b67d5b-5hd2r    1/1       Running   0          35s       group=blue,pod-template-hash=3575623816,run=bluegreen
green-7546d648c4-p7pmz   1/1       Running   0          28s       group=green,pod-template-hash=3102820470,run=bluegreen
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configure&#34;&gt;Configure&lt;/h2&gt;

&lt;p&gt;Create a service that bind both deployments together using the &lt;code&gt;run&lt;/code&gt; label. The expose command need
a deployment object, take anyone, we will override it&amp;rsquo;s selector:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl expose deploy blue --name bluegreen --selector=run=bluegreen
service &amp;quot;bluegreen&amp;quot; exposed

$ kubectl get svc bluegreen -otemplate --template &#39;{{.spec.selector}}&#39;
map[run:bluegreen]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check also the endpoints, it should list both blue and green pods:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get ep bluegreen
NAME         ENDPOINTS                           AGE
bluegreen    172.17.0.11:8000,172.17.0.19:8000   2m

$ kubectl get pod -lrun=bluegreen -owide
NAME                     READY     STATUS    RESTARTS   AGE       IP            NODE
blue-79c9b67d5b-5hd2r    1/1       Running   0          2m        172.17.0.11   192.168.100.99
green-7546d648c4-p7pmz   1/1       Running   0          2m        172.17.0.19   192.168.100.99
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Configure the ingress resource. No need to change the host below, &lt;code&gt;bluegreen.example.com&lt;/code&gt; is fine:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  annotations:
    ingress.kubernetes.io/balance-algorithm: roundrobin
    ingress.kubernetes.io/blue-green-deploy: group=blue=1,group=green=1
    ingress.kubernetes.io/blue-green-mode: pod
    ingress.kubernetes.io/ssl-redirect: &amp;quot;false&amp;quot;
  name: bluegreen
spec:
  rules:
  - host: bluegreen.example.com
    http:
      paths:
      - backend:
          serviceName: bluegreen
          servicePort: 8000
        path: /
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get ing
NAME        HOSTS                   ADDRESS   PORTS     AGE
bluegreen   bluegreen.example.com             80        11s
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;test-blue-green-balance&#34;&gt;Test blue/green balance&lt;/h2&gt;

&lt;p&gt;Lets test! The following snippets use an alias &lt;code&gt;hareq&lt;/code&gt; declared below.
Change &lt;code&gt;IP&lt;/code&gt; to your HAProxy Ingress controller IP address:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ IP=192.168.100.99
$ alias hareq=&#39;echo Running 100 requests...; for i in `seq 1 100`; do
    curl -fsS $IP -H &amp;quot;Host: bluegreen.example.com&amp;quot; | cut -d- -f1
  done | sort | uniq -c&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;BG Mode: pod&lt;/li&gt;
&lt;li&gt;BG Balance: blue=1, green=1&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Replicas: blue=1, green=1&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hareq
Running 100 requests...
50 blue
50 green
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Now changing green replicas to 3 and wait all the replicas to be running.
BG Mode is pod, so the number of replicas will increase the load of the green deployment.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale deploy green --replicas=3
$ kubectl get pod -w
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;BG Mode: pod&lt;/li&gt;
&lt;li&gt;BG Balance: blue=1, green=1&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Replicas: blue=1, green=3&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hareq
Running 100 requests...
25 blue
75 green
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Changing to &lt;em&gt;deploy&lt;/em&gt; mode. This mode targets the balance config to the whole deployment
instead of single pods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; BG mode was added on v0.7. On v0.6, the only supported mode is &lt;code&gt;pod&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl annotate --overwrite ingress bluegreen \
  ingress.kubernetes.io/blue-green-mode=deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;BG Mode: deploy&lt;/li&gt;
&lt;li&gt;BG Balance: blue=1, green=1&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Replicas: blue=1, green=3&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hareq
Running 100 requests...
50 blue
50 green
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Changing now the balance to &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; blue and &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; green:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl annotate --overwrite ingress bluegreen \
  ingress.kubernetes.io/blue-green-deploy=group=blue=1,group=green=2
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;BG Mode: deploy&lt;/li&gt;
&lt;li&gt;BG Balance: blue=1, green=2&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Replicas: blue=1, green=3&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hareq
Running 100 requests...
33 blue
67 green
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;The balance will be the same despite the number of replicas:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale deploy green --replicas=6
$ kubectl get pod -w
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;BG Mode: deploy&lt;/li&gt;
&lt;li&gt;BG Balance: blue=1, green=2&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Replicas: blue=1, green=6&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hareq
Running 100 requests...
33 blue
67 green
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;test-blue-green-selector&#34;&gt;Test blue/green selector&lt;/h2&gt;

&lt;p&gt;Blue/green selector requires HAProxy Ingress controller v0.9 or above.&lt;/p&gt;

&lt;p&gt;Follow the &lt;a href=&#34;#deploying-applications&#34;&gt;deployment&lt;/a&gt; and &lt;a href=&#34;#configure&#34;&gt;configuration&lt;/a&gt;
instructions to deploy the sample application.&lt;/p&gt;

&lt;p&gt;After that, add the following annotation:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl annotate --overwrite ingress bluegreen \
  ingress.kubernetes.io/blue-green-header=x-server:group
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create (or update) the &lt;code&gt;hareq&lt;/code&gt; alias. Change &lt;code&gt;IP&lt;/code&gt; to your HAProxy Ingress controller
IP address:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ IP=192.168.100.99
$ alias hareq=&#39;echo Running 100 requests...; for i in `seq 1 100`; do
    curl -fsS $IP -H &amp;quot;Host: bluegreen.example.com&amp;quot; -H &amp;quot;X-Server: $GROUP&amp;quot; | cut -d- -f1
  done | sort | uniq -c&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Choose &lt;code&gt;blue&lt;/code&gt; group:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GROUP=blue
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The envvar &lt;code&gt;GROUP&lt;/code&gt; will populate the &lt;code&gt;X-Server&lt;/code&gt; header with the value &lt;code&gt;blue&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Run the requests:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ hareq
Running 100 requests...
 100 blue
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Choose &lt;code&gt;green&lt;/code&gt; group:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GROUP=blue
$ hareq
Running 100 requests...
 100 green
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Choose an invalid group, the configured blue/green balance will be used:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl annotate --overwrite ingress bluegreen \
  ingress.kubernetes.io/blue-green-deploy=group=blue=1,group=green=3
$ GROUP=invalid
$ hareq
Running 100 requests...
  25 blue
  75 green
&lt;/code&gt;&lt;/pre&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: External haproxy</title>
      <link>/v0.12/docs/examples/external-haproxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/v0.12/docs/examples/external-haproxy/</guid>
      <description>
        
        
        

&lt;p&gt;This example demonstrates how to configure HAProxy Ingress to manage an external
haproxy instance deployed as a sidecar container. This approach decouple the
controller and the running haproxy version, allowing the sysadmin to update any
of them independently of the other.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;This document requires only a Kubernetes cluster. HAProxy Ingress doesn&amp;rsquo;t need to be
installed, and if so, the installation process should use the
&lt;a href=&#34;/v0.12/v0.12/docs/getting-started/#installation&#34;&gt;Helm chart&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;configure-the-controller&#34;&gt;Configure the controller&lt;/h2&gt;

&lt;p&gt;The easiest and recommended way to configure an external haproxy is using the Helm
chart with a customized values file. Create the &lt;code&gt;haproxy-ingress-values.yaml&lt;/code&gt; file with the
following content:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;controller&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;hostNetwork&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;false&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;config&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;syslog-endpoint&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;stdout&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;syslog-format&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;raw&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;haproxy&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;enabled&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;image&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;      &lt;/span&gt;repository&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;haproxy&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;      &lt;/span&gt;tag&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;2.3.12&lt;/span&gt;-alpine&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Change the hostNetwork to &lt;code&gt;true&lt;/code&gt; if your cluster doesn&amp;rsquo;t provide a service loadbalancer.
These parameters are also configuring an external haproxy and configuring it to log to
stdout.&lt;/p&gt;

&lt;p&gt;Add the HAProxy Ingress Helm repository if using HAProxy Ingress&amp;rsquo; chart for the first time:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm repo add haproxy-ingress https://haproxy-ingress.github.io/charts
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Install or upgrade HAProxy Ingress using the &lt;code&gt;haproxy-ingress-values.yaml&lt;/code&gt; parameters:&lt;/p&gt;

&lt;p&gt;Hint: change &lt;code&gt;install&lt;/code&gt; to &lt;code&gt;upgrade&lt;/code&gt; if HAProxy Ingress is already installed with Helm.&lt;/p&gt;



&lt;div class=&#34;alert alert-primary&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Note&lt;/h4&gt;
We&amp;rsquo;re asking helm to install the latest v0.12 controller version, which will work
on this example, but it&amp;rsquo;s a best practice to use a proper chart version when moving to
staging or production. Check all stable versions with &lt;code&gt;helm search repo haproxy-ingress/ -l&lt;/code&gt;
and update the &lt;code&gt;--version&lt;/code&gt; command-line below.
&lt;/div&gt;


&lt;pre&gt;&lt;code&gt;$ helm install haproxy-ingress haproxy-ingress/haproxy-ingress\
  --create-namespace --namespace=ingress-controller\
  --version 0.12\
  -f haproxy-ingress-values.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check if the controller successfully starts or restarts:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl --namespace ingress-controller get pod -w
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;test&#34;&gt;Test&lt;/h2&gt;

&lt;p&gt;Open two distinct terminals to follow &lt;code&gt;haproxy-ingress&lt;/code&gt; and &lt;code&gt;haproxy&lt;/code&gt; logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl --namespace ingress-controller get pod
NAME                               READY   STATUS    RESTARTS   AGE
haproxy-ingress-6f8848d6fb-gxmrk   2/2     Running   0          13s

$ kubectl --namespace ingress-controller logs -f haproxy-ingress-6f8848d6fb-gxmrk -c haproxy-ingress
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl --namespace ingress-controller logs -f haproxy-ingress-6f8848d6fb-gxmrk -c haproxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Do some &lt;code&gt;curl&lt;/code&gt; to any exposed application, or just use the controller or service loadbalancer
IP like the example below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl 192.168.1.11
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;HAProxy Ingress and the external haproxy should be logging their own events:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;haproxy-ingress&lt;/code&gt; container:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
I0117 17:30:27.282701       6 controller.go:87] HAProxy Ingress successfully initialized
I0117 17:30:27.282743       6 leaderelection.go:243] attempting to acquire leader lease  ingress-controller/ingress-controller-leader-haproxy...
I0117 17:30:27.335674       6 status.go:177] new leader elected: haproxy-ingress-6f8848d6fb-cxb6w
I0117 17:30:27.392372       6 controller.go:321] starting haproxy update id=1
I0117 17:30:27.392463       6 ingress.go:153] using auto generated fake certificate
I0117 17:30:27.437047       6 instance.go:309] haproxy successfully reloaded (external)
I0117 17:30:27.437217       6 controller.go:353] finish haproxy update id=1: parse_ingress=0.143483ms write_maps=0.149637ms write_config=0.971026ms reload_haproxy=43.498718ms total=44.762864ms
I0117 17:30:58.066768       6 leaderelection.go:253] successfully acquired lease ingress-controller/ingress-controller-leader-haproxy
I0117 17:30:58.066867       6 status.go:177] new leader elected: haproxy-ingress-6f8848d6fb-gxmrk
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;haproxy&lt;/code&gt; container:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
192.168.1.10:61116 [17/Jan/2021:17:32:36.050] _front_http _error404/&amp;lt;lua.send-404&amp;gt; 0/0/0/0/0 404 190 - - LR-- 1/1/0/0/0 0/0 &amp;quot;GET / HTTP/1.1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;what-was-changed&#34;&gt;What was changed?&lt;/h2&gt;

&lt;p&gt;The sections below have details of what changed in the deployment compared with a
default installation.&lt;/p&gt;

&lt;h3 id=&#34;sidecar&#34;&gt;Sidecar&lt;/h3&gt;

&lt;p&gt;This example configures 2 (two) new containers in the controllers&amp;rsquo; pod:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;haproxy&lt;/code&gt; is the external haproxy deployment with two mandatory arguments: &lt;code&gt;-S&lt;/code&gt; with the master CLI unix socket, and &lt;code&gt;-f&lt;/code&gt; with the configuration files path&lt;/li&gt;
&lt;li&gt;&lt;code&gt;init&lt;/code&gt;, a Kubernetes&amp;rsquo; &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/init-containers/&#34;&gt;initContainer&lt;/a&gt; used to create an initial and valid haproxy configuration.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;code&gt;haproxy&lt;/code&gt; container references the official Alpine based image &lt;code&gt;haproxy:2.3.12-alpine&lt;/code&gt;,
but can be any other, provided that it isn&amp;rsquo;t older than the minimal required version. The
minimal version for the external HAProxy can be found in the
&lt;a href=&#34;https://github.com/jcmoraisjr/haproxy-ingress/#use-haproxy-ingress&#34;&gt;README&lt;/a&gt; file.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;init&lt;/code&gt; container just copy a minimum and valid &lt;code&gt;haproxy.cfg&lt;/code&gt;. This file is used
to properly starts haproxy and configures its master CLI that HAProxy Ingress uses
to manage the instance.&lt;/p&gt;

&lt;p&gt;A new command-line &lt;code&gt;--master-socket&lt;/code&gt; was also added to the HAProxy Ingress container.
This option enables an external haproxy instance, pointing to the unix socket path
of its master CLI.&lt;/p&gt;

&lt;h3 id=&#34;shared-filesystem&#34;&gt;Shared filesystem&lt;/h3&gt;

&lt;p&gt;HAProxy Ingress sends configuration files to the haproxy instance using a shared
filesystem. A Kubernetes&amp;rsquo; &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#emptydir&#34;&gt;&lt;code&gt;emptyDir&lt;/code&gt;&lt;/a&gt;
works well.&lt;/p&gt;

&lt;p&gt;The following directories must be shared:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/etc/haproxy&lt;/code&gt;: configuration and map files - &lt;code&gt;init&lt;/code&gt; and &lt;code&gt;haproxy-ingress&lt;/code&gt; need write access, &lt;code&gt;haproxy&lt;/code&gt; need read access.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/lib/haproxy&lt;/code&gt;: mostly ssl related files - &lt;code&gt;haproxy-ingress&lt;/code&gt; need write access, &lt;code&gt;haproxy&lt;/code&gt; need read access.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/var/run/haproxy&lt;/code&gt;: unix sockets - &lt;code&gt;haproxy-ingress&lt;/code&gt; and &lt;code&gt;haproxy&lt;/code&gt; need write access.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;liveness-probe&#34;&gt;Liveness probe&lt;/h3&gt;

&lt;p&gt;Default HAProxy Ingress deployment has a liveness probe to an haproxy&amp;rsquo;s health
check URI. This example changes the liveness probe from the HAProxy Ingress
container to the haproxy one.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Metrics</title>
      <link>/v0.12/docs/examples/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/v0.12/docs/examples/metrics/</guid>
      <description>
        
        
        



&lt;div class=&#34;pageinfo pageinfo-primary&#34;&gt;
&lt;p&gt;This is a &lt;code&gt;v0.10&lt;/code&gt; example and need HAProxy Ingress &lt;code&gt;v0.10-snapshot.5&lt;/code&gt; or above&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;This example demonstrates how to configure &lt;a href=&#34;https://prometheus.io&#34;&gt;Prometheus&lt;/a&gt; to collect ingress controller and haproxy metrics, and also to configure a &lt;a href=&#34;https://grafana.com&#34;&gt;Grafana&lt;/a&gt; dashboard to expose these metrics.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;This document has the following prerequisite:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A Kubernetes cluster with a running HAProxy Ingress controller v0.10 or above. See the &lt;a href=&#34;/v0.12/v0.12/docs/getting-started/&#34;&gt;getting started&lt;/a&gt; guide.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;configure-the-controller&#34;&gt;Configure the controller&lt;/h2&gt;

&lt;p&gt;HAProxy Ingress by default does not configure the haproxy&amp;rsquo;s prometheus exporter. The patch below configures the haproxy&amp;rsquo;s internal prometheus exporter in the port &lt;code&gt;9105&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl --namespace ingress-controller patch configmap haproxy-ingress -p &#39;{&amp;quot;data&amp;quot;:{&amp;quot;prometheus-port&amp;quot;:&amp;quot;9105&amp;quot;}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following patch adds ports &lt;code&gt;9105&lt;/code&gt; and &lt;code&gt;10254&lt;/code&gt; to the HAProxy Ingress container. The port declaration is used by the Prometheus&amp;rsquo; service discovery:&lt;/p&gt;

&lt;p&gt;Note: this patch will restart the controller!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl --namespace ingress-controller patch deployment haproxy-ingress -p &#39;{&amp;quot;spec&amp;quot;:{&amp;quot;template&amp;quot;:{&amp;quot;spec&amp;quot;:{&amp;quot;containers&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;haproxy-ingress&amp;quot;,&amp;quot;ports&amp;quot;:[{&amp;quot;name&amp;quot;:&amp;quot;exporter&amp;quot;,&amp;quot;containerPort&amp;quot;:9105},{&amp;quot;name&amp;quot;:&amp;quot;ingress-stats&amp;quot;,&amp;quot;containerPort&amp;quot;:10254}]}]}}}}&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;deploy-prometheus&#34;&gt;Deploy Prometheus&lt;/h2&gt;

&lt;p&gt;This will create a Prometheus deployment with no resource limits, a configuration file which will scrape haproxy and also HAProxy Ingress metrics every &lt;code&gt;10s&lt;/code&gt;, and also a role and rolebinding which allows Prometheus to discover haproxy and controller endpoints using k8s:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://haproxy-ingress.github.io/docs/examples/metrics/prometheus.yaml
&lt;/code&gt;&lt;/pre&gt;



&lt;div class=&#34;alert alert-primary&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Note&lt;/h4&gt;
This deployment has no persistent volume, so all the collected metrics will be lost if the pod is recreated.
&lt;/div&gt;




&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Warning&lt;/h4&gt;
&lt;p&gt;If HAProxy Ingress wasn&amp;rsquo;t deployed with Helm, change the following line in the &lt;code&gt;configmap/prometheus-cfg&lt;/code&gt; resource, jobs &lt;code&gt;haproxy-ingress&lt;/code&gt; and &lt;code&gt;haproxy-exporter&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-diff&#34; data-lang=&#34;diff&#34;&gt;       relabel_configs:
&lt;span style=&#34;color:#a40000&#34;&gt;-      - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_instance]
&lt;/span&gt;&lt;span style=&#34;color:#a40000&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;+      - source_labels: [__meta_kubernetes_pod_label_run]
&lt;/span&gt;&lt;span style=&#34;color:#00a000&#34;&gt;&lt;/span&gt;         regex: haproxy-ingress
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will ensure that Prometheus finds the controller pods.&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;Check if Prometheus is up and running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl --namespace ingress-controller get pod -lrun=prometheus -w
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check also if Prometheus found the haproxy and the controller endpoints:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl --namespace ingress-controller port-forward svc/prometheus 9090:9090
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Open &lt;a href=&#34;http://127.0.0.1:9090/targets&#34;&gt;localhost:9090/targets&lt;/a&gt; in your browser, all haproxy and controller instances should be listed, up, and green.&lt;/p&gt;

&lt;h2 id=&#34;deploy-grafana&#34;&gt;Deploy Grafana&lt;/h2&gt;

&lt;p&gt;The following instruction will create a Grafana deployment with no resource limit, and also its service:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl create -f https://haproxy-ingress.github.io/docs/examples/metrics/grafana.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check if Grafana is up and running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl --namespace ingress-controller get pod -lrun=grafana -w
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create the ingress which will expose Grafana. Change &lt;code&gt;HOST&lt;/code&gt; below to a domain of the cluster, or just change the inner IP number to the IP of the HAProxy Ingress node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;HOST=grafana.192.168.1.1.nip.io
kubectl create -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: grafana
  namespace: ingress-controller
spec:
  rules:
  - host: $HOST
    http:
      paths:
      - backend:
          serviceName: grafana
          servicePort: 3000
        path: /
  tls:
  - hosts:
    - $HOST
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configure-dashboard&#34;&gt;Configure dashboard&lt;/h2&gt;

&lt;p&gt;Now its time to see what Prometheus is collecting. Grafana can be acessed in the &lt;code&gt;HOST&lt;/code&gt; used to configure the ingress object.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Log in to Grafana, user is &lt;code&gt;admin&lt;/code&gt; and the first password is &lt;code&gt;admin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Create a Prometheus data source, the endpoint is &lt;code&gt;http://prometheus:9090&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Click the big &lt;code&gt;+&lt;/code&gt; plus sign in the left side, and Import &lt;a href=&#34;https://grafana.com/grafana/dashboards/12056&#34;&gt;this&lt;/a&gt; dashboard&lt;/li&gt;
&lt;/ul&gt;



&lt;div class=&#34;alert alert-primary&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Note&lt;/h4&gt;
This deployment has no persistent volume, so all customizations will be lost if the pod is recreated.
&lt;/div&gt;


&lt;p&gt;If everything worked as expected, you should see a dashboard like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/v0.12/docs/examples/metrics/dashboard-1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;compatibility&#34;&gt;Compatibility&lt;/h2&gt;

&lt;p&gt;Check if you have any of these scenarios and adjust Prometheus or Grafana accordingly.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Grafana &amp;lt; 6.5&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Use Grafana 6.5 or higher. Although an older Grafana won&amp;rsquo;t refuse to install this dashboard, some widgets will not render as expected on older versions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The &lt;code&gt;hostname&lt;/code&gt; label&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;All the metrics of this dashboard are grouped by the &lt;code&gt;hostname&lt;/code&gt; label. If you already have Prometheus and HAProxy Exporter, ensure that the &lt;code&gt;hostname&lt;/code&gt; label uniquely identifies all HAProxy Ingress instances, just like the proposed Prometheus configuration does.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The &lt;code&gt;_front__tls&lt;/code&gt; proxy&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This dashboard is designed to work without the &lt;code&gt;_front__tls&lt;/code&gt; proxy, such proxy is created whenever ssl-passthrough is used or timeout client is configured in the ingress object. Just to be sure, check if &lt;code&gt;grep _front__tls /etc/haproxy/haproxy.cfg&lt;/code&gt; find a proxy declaration. If the &lt;code&gt;_front__tls&lt;/code&gt; proxy exists, edit the dashboard and change the variable &lt;code&gt;$public_frontend&lt;/code&gt; to the following new value: &lt;code&gt;(_tcp_.*|_front_http|_front__tls)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using the Prometheus&amp;rsquo; HAProxy Exporter&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This dashboard works with haproxy&amp;rsquo;s internal Prometheus exporter. Follow these steps to adjust the scrape config and the dashboard if using &lt;a href=&#34;https://github.com/prometheus/haproxy_exporter&#34;&gt;Prometheus&amp;rsquo; HAProxy Exporter&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Change the metric name of &amp;ldquo;Backend status / Top 5 max/avg connection time&amp;rdquo; to &lt;code&gt;haproxy_backend_http_connect_time_average_seconds&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Add this &lt;code&gt;metric_relabel_config&lt;/code&gt; in the Prometheus configuration:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;metric_relabel_configs&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;source_labels&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;frontend&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;regex&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;(.+)&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;target_label&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;proxy&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;-&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;source_labels&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;[&lt;/span&gt;backend&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;]&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;regex&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;(.+)&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;    &lt;/span&gt;target_label&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;proxy&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;test&#34;&gt;Test&lt;/h2&gt;

&lt;p&gt;Lets make some noise and see what the dashboard tell us about our HAProxy Ingress cluster.&lt;/p&gt;

&lt;p&gt;Deploy a demo application and a custom (self-signed) certificate:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;openssl req -x509 -subj &amp;quot;/CN=whoami.localdomain&amp;quot; -nodes -days 30 -newkey rsa:2048 -keyout /tmp/w.key -out /tmp/w.crt
kubectl --namespace default create secret tls whoami --cert /tmp/w.crt --key /tmp/w.key
rm -fv /tmp/w.crt /tmp/w.key
kubectl --namespace default create deploy whoami --image jcmoraisjr/whoami
kubectl --namespace default scale deploy whoami --replicas=4
kubectl --namespace default expose deploy whoami --port 8000
kubectl create -f https://haproxy-ingress.github.io/docs/examples/metrics/whoami-ingress.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check if the app is up and running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl --namespace default get pod -lapp=whoami -w
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Download &lt;a href=&#34;https://github.com/tsenart/vegeta/releases&#34;&gt;vegeta&lt;/a&gt; and place it in the path.&lt;/p&gt;

&lt;p&gt;Make a test and check if everything is working as expected. Change IP below to the IP of a HAProxy Ingress node:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IP=192.168.1.1
echo &amp;quot;GET http://${IP}&amp;quot; |\
  vegeta attack -duration=1s -rate=1 -header &amp;quot;Host: whoami.localdomain&amp;quot; -keepalive=true |\
  vegeta report
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see something like this. The most important part is Success ratio=100% and an Error Set empty:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;...
Success       [ratio]                    100.00%
Status Codes  [code:count]               200:1
Error Set:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now the real test. Adjust the duration and rate (number of requests per second) if needed. A dual core VM dedicated to HAProxy Ingress should accept a few thousands requests per second. Lets configure &lt;code&gt;200&lt;/code&gt; which should move some lines in the dashoard:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IP=192.168.1.1
echo &amp;quot;GET http://${IP}&amp;quot; |\
  vegeta attack -duration=5m -rate=200 -header &amp;quot;Host: whoami.localdomain&amp;quot; -keepalive=true |\
  vegeta report
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Follow the dashboard while the test is running. Most metrics have its resolution in &lt;code&gt;1m&lt;/code&gt; (one minute) so you should wait this time to see the correct conn/s, rps, proc use and so on.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s the impact of not use keepalive? Try the same test, changing only &lt;code&gt;-keepalive&lt;/code&gt; to  &lt;code&gt;false&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IP=192.168.1.1
echo &amp;quot;GET http://${IP}&amp;quot; |\
  vegeta attack -duration=5m -rate=200 -header &amp;quot;Host: whoami.localdomain&amp;quot; -keepalive=false |\
  vegeta report
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Last test: what about TLS connections without keepalive? Change &lt;code&gt;http&lt;/code&gt; to &lt;code&gt;https&lt;/code&gt; and add &lt;code&gt;-insecure&lt;/code&gt; command-line option to Vegeta:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;IP=192.168.1.1
echo &amp;quot;GET https://${IP}&amp;quot; |\
  vegeta attack -insecure -duration=5m -rate=200 -header &amp;quot;Host: whoami.localdomain&amp;quot; -keepalive=false |\
  vegeta report
&lt;/code&gt;&lt;/pre&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: ModSecurity</title>
      <link>/v0.12/docs/examples/modsecurity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/v0.12/docs/examples/modsecurity/</guid>
      <description>
        
        
        

&lt;p&gt;This example demonstrates how to configure ModSecurity
web application firewall on HAProxy Ingress controller.&lt;/p&gt;

&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;

&lt;p&gt;This document has the following prerequisites:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A Kubernetes cluster with a running HAProxy Ingress controller. See the &lt;a href=&#34;https://github.com/jcmoraisjr/haproxy-ingress/tree/master/examples/setup-cluster.md#five-minutes-deployment&#34;&gt;five minutes deployment&lt;/a&gt; or the &lt;a href=&#34;https://github.com/jcmoraisjr/haproxy-ingress/tree/master/examples/deployment&#34;&gt;deployment example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ingress-controller&lt;/code&gt; namespace, the default of the five minutes deployment&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;deploying-agent&#34;&gt;Deploying agent&lt;/h2&gt;

&lt;p&gt;A ModSecurity agent can be deployed in a number of ways: as a sidecar container
in the same HAProxy Ingress deployment/daemonset resource, as a standalone container
in the same host of ingress, or in dedicated host(s), inside or outside a k8s cluster.
The steps below will deploy ModSecurity in some dedicated hosts of a k8s cluster,
adjust the steps to fit your need.&lt;/p&gt;

&lt;p&gt;The ModSecurity agent used is &lt;a href=&#34;https://github.com/jcmoraisjr/modsecurity-spoa&#34;&gt;jcmoraisjr/modsecurity-spoa&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Create the ModSecurity agent daemonset:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f https://haproxy-ingress.github.io/resources/modsecurity-daemonset.yaml
daemonset &amp;quot;modsecurity-spoa&amp;quot; created
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Select the node(s) where ModSecurity agent should run:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get node
NAME             STATUS    AGE       VERSION
192.168.100.99   Ready     102d      v1.9.2
...

$ kubectl label node 192.168.100.99 waf=modsec
node &amp;quot;192.168.100.99&amp;quot; labeled
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check if the agent is up and running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n ingress-controller get pod -lrun=modsecurity-spoa -owide
NAME                     READY     STATUS    RESTARTS   AGE       IP               NODE
modsecurity-spoa-pp6jz   1/1       Running   0          7s        192.168.100.99   192.168.100.99
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configuring-haproxy-ingress&#34;&gt;Configuring HAProxy Ingress&lt;/h2&gt;

&lt;p&gt;Add the ConfigMap key &lt;code&gt;modsecurity-endpoints&lt;/code&gt; with a comma-separated list of &lt;code&gt;IP:port&lt;/code&gt;
of the ModSecurity agent server(s). The default port number of the agent is &lt;code&gt;12345&lt;/code&gt;.
A &lt;code&gt;kubectl -n ingress-controller edit configmap haproxy-ingress&lt;/code&gt; should work.&lt;/p&gt;

&lt;p&gt;Example of a ConfigMap content if ModSecurity agents has IPs &lt;code&gt;192.168.100.99&lt;/code&gt; and
&lt;code&gt;192.168.100.100&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;apiVersion&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;v1&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;&lt;/span&gt;data&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;modsecurity-endpoints&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;192.168.100.99&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;12345&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;192.168.100.100&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;12345&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;  &lt;/span&gt;...&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt;&lt;/span&gt;kind&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;:&lt;/span&gt;&lt;span style=&#34;color:#f8f8f8;text-decoration:underline&#34;&gt; &lt;/span&gt;ConfigMap&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;test&#34;&gt;Test&lt;/h2&gt;

&lt;p&gt;Deploy any application:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run echo \
  --image=gcr.io/google_containers/echoserver:1.3 \
  --port=8080 \
  --expose
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; and create its ingress resource. Remember to annotate waf as &lt;code&gt;modsecurity&lt;/code&gt;.
No need to use a valid domain, &lt;code&gt;echo.domain&lt;/code&gt; below is fine:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-console&#34; data-lang=&#34;console&#34;&gt;$ kubectl create -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  annotations:
    ingress.kubernetes.io/ssl-redirect: &amp;#34;false&amp;#34;
    ingress.kubernetes.io/waf: &amp;#34;modsecurity&amp;#34;
  name: echo
spec:
  rules:
  - host: echo.domain
    http:
      paths:
      - path: /
        backend:
          serviceName: echo
          servicePort: 8080
EOF&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Test with a simple request. Change the IP below to the IP of your Ingress controller:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl -I 192.168.100.99 -H &#39;Host: echo.domain&#39;
HTTP/1.1 200 OK
Server: nginx/1.9.11
Date: Sun, 27 May 2018 23:28:58 GMT
Content-Type: text/plain
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Test now with a malicious request:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -i &#39;192.168.100.99?p=/etc/passwd&#39; -H &#39;Host: echo.domain&#39;
HTTP/1.0 403 Forbidden
Cache-Control: no-cache
Connection: close
Content-Type: text/html

&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;403 Forbidden&amp;lt;/h1&amp;gt;
Request forbidden by administrative rules.
&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Check the agent logs:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl -n ingress-controller get pod -lrun=modsecurity-spoa
NAME                     READY     STATUS    RESTARTS   AGE
modsecurity-spoa-5g5h2   1/1       Running   0          1h
...

$ kubectl -n ingress-controller logs --tail=10 modsecurity-spoa-5g5h2
...
1527464273.942819 [00] [client 127.0.0.1] ModSecurity: Access denied with code 403 (phase 2). Matche
d phrase &amp;quot;etc/passwd&amp;quot; at ARGS:p. [file &amp;quot;/etc/modsecurity/owasp-modsecurity-crs/rules/REQUEST-930-APP
LICATION-ATTACK-LFI.conf&amp;quot;] [line &amp;quot;108&amp;quot;] [id &amp;quot;930120&amp;quot;] [rev &amp;quot;4&amp;quot;] [msg &amp;quot;OS File Access Attempt&amp;quot;] [data
 &amp;quot;Matched Data: etc/passwd found within ARGS:p: /etc/passwd&amp;quot;] [severity &amp;quot;CRITICAL&amp;quot;] [ver &amp;quot;OWASP_CRS/
3.0.0&amp;quot;] [maturity &amp;quot;9&amp;quot;] [accuracy &amp;quot;9&amp;quot;] [tag &amp;quot;application-multi&amp;quot;] [tag &amp;quot;language-multi&amp;quot;] [tag &amp;quot;platfor
m-multi&amp;quot;] [tag &amp;quot;attack-lfi&amp;quot;] [tag &amp;quot;OWASP_CRS/WEB_ATTACK/FILE_INJECTION&amp;quot;] [tag &amp;quot;WASCTC/WASC-33&amp;quot;] [tag
 &amp;quot;OWASP_TOP_10/A4&amp;quot;] [tag &amp;quot;PCI/6.5.4&amp;quot;] [hostname &amp;quot;ingress.localdomain&amp;quot;] [uri &amp;quot;http://echo.domain/&amp;quot;] [
unique_id &amp;quot;&amp;quot;]
...
&lt;/code&gt;&lt;/pre&gt;

      </description>
    </item>
    
  </channel>
</rss>
